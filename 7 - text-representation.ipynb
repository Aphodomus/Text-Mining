{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text Representation**\n",
    "É uma forma de numericamente representar símbolos, letras, palavras, sentenças e documentos para fazer com que esses itens/textos sejam matematicamente computáveis. Geralmente representamos textos como um vetor numérico, um vetor de números, de forma que se a gente precisar comparar dois vetores numéricos, podemos utilizar ferramentas do arcabouço matemáticos para verificar a similaridade entre dois vetores por meio do cosseno, como mostrado abaixo.\n",
    "Além de habilitar essa computação matemática, podemos também criar modelos de linguagem (Language Models). Capazes de predizer uma palavra dada uma sequência de palavras anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [2 1 2 3 2 9]\n",
      "B: [3 4 2 4 5 5]\n",
      "Cosine Similarity: 0.8188504723485274\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "A = np.array([2, 1, 2, 3, 2, 9])\n",
    "B = np.array([3, 4, 2, 4, 5, 5])\n",
    "\n",
    "print(\"A:\", A)\n",
    "print(\"B:\", B)\n",
    "\n",
    "cosine = np.dot(A, B)/(norm(A) * norm(B))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Probabilistics Models**\n",
    "São os modelos mais tradicionais, foram desenvolvidos a algumas décadas. Ele basicamente aprende a probabilidade da ocorrência de uma palavra com base em um exemplo de texto que ele pega. Pode prever a próxima palavra baseado em uma sequência pequena de palavras. Representa uma probabilidade condicionada da próxima palavra dada todas as palavras anteriores. Um exemplo é o Bag of Words (BoW) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Topic Models**\n",
    "Também são conhecidos como probabilistic topic models, são um caso particular dos modelos probabilisticos, pois na verdade eles descobrem tópicos latentes a partir de texto. Ele basicamente vai clusterizar as palavras semelhantes e descobrir tópicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Neural Models**\n",
    "É bem diferente dos modelos anteriores. A ideia é prever representações contínuas de embeding de palavras e reduzir a dimensionalidade. Ajudam a incorporar mais semântica na representação, ajudam a resolver problemas como duplicidade de significado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BoW (Bag of Words)**\n",
    "Esse modelo é probabilistico, simplifica a representação das palavras levando em consideração que as palavras são independentes entre si, independente das anteriores. É um modelo muito simplse e com muitas limitações. Representamos um texto em um vetor n-dimensional. Por exemplo, o texto \"the who is the band\", é composto pelas seguintes palavras: \"band\", \"is\", \"the\" and \"who\". Uma representação de texto usando o BoW é [1.0, 1.0, 2.0, 1.0], onde cada dimensão corresponde a uma palavra no vocabulário e cada número real é a frequência da palavra no texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **One-Hot Encoding**\n",
    "É o processo de gerar vetores usados para distinguir textos um dos outros. O vetor consiste em representações com 0 e 1, são usadas para identificar unicamente um texto. Por exemplo, o texto \"the who is the band\" uma representação para cada uma das palavras é: \n",
    "- \"band\" como [1, 0, 0, 0].\n",
    "- \"is\" como [0, 1, 0, 0].\n",
    "- \"the\" como [0, 0, 1, 0].\n",
    "- \"who\" como [0, 0, 0, 1].\n",
    "\n",
    "Onde cada dimensão representa uma palavra do vocabulário."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BoW with One-Hot Encoding**\n",
    "Podemos utilizar essa abordagem para representar textos maiores, como documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C has 3 texts:\n",
      "t1 = The who is the band!\n",
      "t2 = who is the band?\n",
      "t3 = The band who plays the who.\n"
     ]
    }
   ],
   "source": [
    "C = ['The who is the band!', 'who is the band?', 'The band who plays the who.']\n",
    "\n",
    "print('C has %d texts:' % len(C))\n",
    "for i in range(len(C)):\n",
    "    print(f\"t{i+1} = {C[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The who is the band!', 'who is the band?', 'The band who plays the who.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V has 5 words: ['band', 'is', 'plays', 'the', 'who']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_tokens(text):\n",
    "    # ignore = ['a', 'the', 'is']\n",
    "    tokens = re.sub(\"[^\\w]\", \" \", text).split()\n",
    "    return [w.lower() for w in tokens]\n",
    "\n",
    "def tokenize(texts):\n",
    "    words = []\n",
    "    \n",
    "    for text in texts:\n",
    "        w = get_tokens(text)\n",
    "        words.extend(w)\n",
    "        words = sorted(list(set(words)))\n",
    "    \n",
    "    return words\n",
    "\n",
    "V = tokenize(C)\n",
    "print(f\"V has {len(V)} words: {V}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['band', 'is', 'plays', 'the', 'who']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The who is the band!', 'who is the band?', 'The band who plays the who.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The who is the band! = [1. 1. 0. 1. 1.]\n",
      "who is the band? = [1. 1. 0. 1. 1.]\n",
      "The band who plays the who. = [1. 0. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "for text in C:\n",
    "    words = get_tokens(text)\n",
    "    bag_vector = numpy.zeros(len(V))\n",
    "    \n",
    "    for w in words:\n",
    "        for i, word in enumerate(V):\n",
    "            if word == w:\n",
    "                bag_vector[i] = 1\n",
    "                \n",
    "    print(f\"{text} = {numpy.array(bag_vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É uma representação limitada, pois não considera a correlação entre as palavras. Ou seja, a semântica não é bem representada, sendo um método não tão interessante para Text Mining."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
