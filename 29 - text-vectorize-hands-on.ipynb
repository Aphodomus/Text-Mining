{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text Vectorization**\n",
    "Os algoritmos de machine learning operam em um espaço de recursos numérico, esperando a entrada como uma matriz bidimensional onde as linhas são instâncias e as colunas são recursos. Para realizar o aprendizado de máquina em texto, precisamos transformar nossos documentos em representações vetoriais para que possamos aplicar o aprendizado de máquina numérico.\n",
    "\n",
    "A representação numérica de documentos nos dá a capacidade de realizar análises significativas e também cria as instâncias nas quais os algoritmos de aprendizado de máquina operam.Na análise de texto, as instâncias são documentos ou enunciados inteiros, que podem variar em comprimento, desde citações ou tweets até livros inteiros, mas cujos vetores têm sempre comprimento uniforme.Cada propriedade da representação vetorial é um recurso . Para texto, os recursos representam atributos e propriedades de documentos – incluindo seu conteúdo, bem como meta atributos, como comprimento do documento, autor, fonte e data de publicação. Quando considerados em conjunto, os recursos de um documento descrevem um espaço de recursos multidimensional no qual os métodos de aprendizado de máquina podem ser aplicados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bag of Words**\n",
    "Para vetorizar um corpus com uma abordagem bag-of-words (BOW), representamos cada documento do corpus como um vetor cujo comprimento é igual ao vocabulário do corpus.\n",
    "\n",
    "<div align=\"center\" style=\"margin-top: 40px;\">\n",
    "    <img src=\"./images/bow2.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example**\n",
    "O \"tokenize\" método realiza uma normalização leve, eliminando a pontuação usando o string.punctuationconjunto de caracteres e definindo o texto como minúsculo. Esta função também realiza alguma redução de recursos usando para \"SnowballStemmerremover\" afixos como pluralidade (“morcegos” e “morcego” são o mesmo token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The elephant sneezed at the sight of potatoes.',\n",
       " 'Bats can see via echolocation. See the bat sight sneeze!',\n",
       " 'Wondering, she opened the door to the studio.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "def tokenize(text):\n",
    "    stem = nltk.stem.SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation: continue\n",
    "        yield stem.stem(token)\n",
    "\n",
    "corpus = [\n",
    "    \"The elephant sneezed at the sight of potatoes.\",\n",
    "    \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
    "    \"Wondering, she opened the door to the studio.\",\n",
    "]\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Frequency Vectors**\n",
    "Os vetores de frequência são os modelos mais simples, onde simplesmente preenchemos o vetor com a frequência de cada palavra conforme aparece no documento.\n",
    "\n",
    "<div align=\"center\" style=\"margin-top: 40px;\">\n",
    "    <img src=\"./images/bow3.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 23 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectors_count = vectorizer.fit_transform(corpus)\n",
    "vectors_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observações: Os vetores podem se tornar extremamente esparsos, principalmente à medida que os vocabulários ficam maiores, o que pode ter um impacto significativo na velocidade e no desempenho dos modelos de aprendizado de máquina. Para corpora muito grandes, é recomendado usar o Scikit-Learn HashingVectorizer, que usa um truque de hashing para encontrar o nome da string do token para apresentar o mapeamento do índice. Isso significa que ele usa muito pouca memória e pode ser dimensionado para grandes conjuntos de dados, pois não precisa armazenar todo o vocabulário e é mais rápido de selecionar e ajustar, pois não há estado. No entanto, não há transformação inversa (de vetor para texto), pode haver colisões e não há ponderação de frequência inversa do documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **One-Hot Encoding**\n",
    "Os vetores de frequência são os modelos mais simples, onde simplesmente preenchemos o vetor com a frequência de cada palavra conforme aparece no documento.\n",
    "\n",
    "<div align=\"center\" style=\"margin-top: 40px;\">\n",
    "    <img src=\"./images/bow3.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
