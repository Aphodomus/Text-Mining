{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vectorization**\n",
    "o processamento de linguagem natural (PNL), frequentemente falamos sobre vetorização de texto – representando palavras, frases ou até unidades maiores de texto como vetores (ou “incorporação de vetores”). Outros tipos de dados, como imagens, sons e vídeos, também podem ser codificados como vetores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Count-Based Text Vectorization**\n",
    "Um vetor é uma estrutura de dados semelhante a uma lista ou array, no qual para fins de representação de uma frase por exemplo, podemos representar como simplesmente sendo a sucessão de valores, com o número de palavras representando a dimensionalidade do vetor. Ou seja, quanto mais palavras, maior vai ser o vetor. Fornece um formato no qual os computadores podem acessar facilmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Bag of Words**\n",
    "É um dos métodos mais simples para texto, é uma representação de BoW. BoW possui o comprimento de todo o vocabulário, ou seja, o conjunto de palavras únicas no corpus. Os valores do vetor representam a frequência com que cada palavra aparece em uma determinada passagem de texto:\n",
    "\n",
    "<div align=\"center\" style=\"margin-top: 40px;\">\n",
    "    <img src=\"./images/bow.png\" alt=\"Alt text\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **TF-IDF**\n",
    "É uma técnica de vetorização de texto ponderada, que tenta atribuir pontuações de relevância mais altas a palavras que ocorrem em menos documentos dentro do corpus. Para esse fim, o TF-IDF mede a frequência de uma palavra num texto em relação à sua frequência global no corpus.\n",
    "Pense em um documento que mencione a palavra \"laranjas\" com alta frequência. O TF-IDF anasilará todos os outros documentos do corpus. Se \"laranjas\" ocorrer em muitos documentos, então não é um termo muito significativo e recebe um peso menor no vetor de texto do TF-IDF. Entretanto, se a palavra \"laranja\" ocorrer em apenas alguns poucos documentos, ela vai ser considerada como um termo distintivo e importante. Ou seja, vai ajudar a caracterizar o documento dentro do corpus e como tal vai recebe um valor maior no vetor, uma pontuação maior e com mais peso.\n",
    "- Embora o TF-IDF seja melhor que o BoW, ele tem algumas deficiências. Por exemplo, ele não aborda o fato de que, em documentos curtos, mesmo a simples menção de uma palavra pode significar que o termo é altamente relevante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **BM25**\n",
    "O BM25 foi introduzido para resolver os problemas do BoW e TF-IDF. É uma melhoria em relação ao TF-IDF, pois leva em consideração a extensão do documento. Também diminui o efeito de ter muitas ocorrências de uma palavra em um documento. Como os métodos de BoW produzem vetores longos que contêm muitos zeros, eles são frequentemente chamados de \"vetores esparsos\", ou seja, vetores com muitos valores iguais a 0 que não contribuem em nada. Além de serem independentes de linguagem, vetores esparsos são rápidos de calcular e comparar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word2Vec: Inferindo Significado do Contexto**\n",
    "É uma técnica de codificação mais recente que visa capturar não apenas propriedades lexicais, mas também as semânticas das palavras. As palavras são mais do que apenas uma coleção de letras. Como falantes de uma língua, podemos compreender o que uma palavra significa e como usá-la em uma frase. Em suma, entenderiamos a sua semântica. Os métodos esparsos baseados em contagem que vimos anteriormente **não levam em conta o significado das palavras ou frases que nosso sistema cognitivo processa**.\n",
    "O Word2Vec foi criado para representar as propriedades semânticas e sintáticas das palavras por meio de \"Embeddings\". Word2Vec segue a ideia de que o significado das palavras reside em suas propriedades distributivas - os contextos em que uma palavra é usada. Existem duas implementações principais do Word2Vec, o CBOW e o Skip-Gram.\n",
    "- Ambos treinam uma rede neural superficial para representar palavras como vetores de características de comprimento variável (normalmente 300). Esses vetores são densos , o que significa que consistem principalmente em valores de ponto flutuante, em vez de zeros.\n",
    "\n",
    "No espaço de incorporação Word2Vec de alta dimensão, palavras semelhantes ficam próximas umas das outras. Por exemplo, esperaríamos que as palavras “laranja” e “maçã” estivessem próximas, enquanto, digamos, “casa” ou “nave espacial” deveriam estar mais distantes do par. A similaridade textual semântica é medida por meio de uma métrica de distância, normalmente similaridade de cosseno . Embora seja virtualmente impossível imaginar 300 dimensões de cabeça, os embeddings podem ser reduzidos a apenas duas dimensões para fins de visualização.\n",
    "- Frequentemente, desejaremos codificar o significado de passagens mais longas, como frases de documentos inteiros. **Poderíamos adicionar ou calcular a média dos vetores de palavras individuais para produzir um vetor para todo o texto**. Outra abordagem é o modelo Doc2Vec , que foi treinado de maneira semelhante ao Word2Vec, apenas em documentos e não em palavras.\n",
    "\n",
    "No entanto, apesar da codificação das propriedades semânticas das palavras, a vetorização do texto resultante ainda deixa muito a desejar. Por um lado, as codificações Word2Vec são rígidas: **uma determinada palavra será sempre codificada da mesma maneira, independentemente de sua posição na frase ou de ter múltiplos significados**. O modelo também não tem como explicar palavras desconhecidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformers: Trazendo Semântica Profunda para Vetorização de Texto**\n",
    "O lendário BERT (Bidirecional Encoder Representations from Transformers) supera os obstáculos apresentados pelos métodos anteriores. O BERT é capaz de produzir vetores de palavras contextualizados, codificando a posição de uma palavra no texto, além da própria palavra. Além disso, esta abordagem pode levar em conta palavras desconhecidas, bem como palavras com múltiplos significados.\n",
    "O sucesso do BERT é baseado em sua arquitetura transformer, bem como nas grandes quantidades de dados que ele usa para aprender. Durante o treinamento, o BERT \"lê\" toda a Wikipédia em inglês e o BooksCorpus, uma grande coleção de romances inéditos. Os sucessores do BERT, como RoBERTa, são treinados em coleções de texto ainda maiores. Dessa forma, os modelos de linguagem baseados no Transformer aprender uma representação da linguagem mais profunda e sensível ao contexto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
